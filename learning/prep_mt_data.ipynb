{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODOS\n",
      "# - more tests and sanity checks\n",
      "# - add cross validation harness\n",
      "# - add random instance selection\n",
      "# - load serialized indices, run random indexing, and print scores into:\n",
      "# en-es.sim_scores\n",
      "\n",
      "# TODO: this doesn't work inside the ipython notebook for some reason\n",
      "# %run /home/chris/projects/random_indexing/python/wmt2014/quality_estimation/test_random_indexing.py\n",
      "\n",
      "# split that file into 10 parts (randomly) and do cross-validation\n",
      "\n",
      "# SETUP PARAMS\n",
      "\n",
      "\n",
      "# wmt12 data\n",
      "wmt_effort_scores = '/home/chris/projects/random_indexing/python/wmt2014/quality_estimation/data/wmt_2012_data/QualityEstimation/training_set_annotations/target_system.effort'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check: are our features any good? - compare high-scoring pairs to their QE scores\n",
      "# 10.3.14 - Answer: currently no :-(\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import unittest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODOS\n",
      "# - add cross validation harness\n",
      "# - add random instance selection\n",
      "# - load serialized indices\n",
      "import re\n",
      "\n",
      "# run the indexing and serialization code here\n",
      "# Working - split the data for cross-validation\n",
      "similarity_data_path = '/home/chris/projects/random_indexing/python/wmt2014/quality_estimation/output/'\n",
      "quest_data_path = '/home/chris/projects/quest-new/learning/data/features/wmt2014_qe_experiments/'\n",
      "\n",
      "# semantic_scores = open(similarity_data_path + 'en-es.sim_scores', 'r').readlines()\n",
      "# print(len(semantic_scores))\n",
      "# human_scores = open(quest_data_path + 'train.effort', 'r').read().splitlines()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# open  the new features\n",
      "# rand_train = open(similarity_data_path + 'en-es.sim_scores.train', 'r').read().splitlines()\n",
      "# rand_test = open(similarity_data_path + 'en-es.sim_scores.test', 'r').read().splitlines()\n",
      "semantic_scores = open(similarity_data_path + 'en-es.sim_scores', 'r').read().splitlines()\n",
      "rand_train=semantic_scores[:3000]\n",
      "rand_test =semantic_scores[3000:]\n",
      "# open  the existing features\n",
      "# these are features learned and output by the quest framework\n",
      "quest_train = open(quest_data_path + 'train.15.en-es.tsv', 'r').read().splitlines()\n",
      "quest_test = open(quest_data_path + 'test.15.en-es.tsv', 'r').read().splitlines()\n",
      "\n",
      "train_scores = open(quest_data_path + 'train.effort', 'r').read().splitlines()\n",
      "test_scores = open(quest_data_path + 'test.effort', 'r').read().splitlines()\n",
      "\n",
      "# PREP THE TEST DATA\n",
      "new_train = [q + '\\t' + r for q,r in zip(quest_train, rand_train)]\n",
      "new_test = [q + '\\t' + r for q,r in zip(quest_test, rand_test)]\n",
      "# print (new_train[5:])\n",
      "# print (new_test[5:])\n",
      "# without the extra features - Quest file only\n",
      "# new_train = quest_train\n",
      "# new_test = quest_test\n",
      "\n",
      "# JUST the extra feature\n",
      "new_train = rand_train\n",
      "new_test = rand_test\n",
      "\n",
      "train_len = len(new_train)\n",
      "test_len = len(new_test)\n",
      "\n",
      "# sanity check - random vals between 0-1\n",
      "#new_train = [np.random.random() for l in range(train_len)]\n",
      "#new_test = [np.random.random() for l in range(test_len)]\n",
      "\n",
      "print (\"rand_train length: %i rand_test length: %i\" % (len(rand_train), len(rand_test)))\n",
      "print (\"quest_train length: %i quest_test length: %i\" % (len(quest_train), len(quest_test)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "rand_train length: 3000 rand_test length: 816\n",
        "quest_train length: 3000 quest_test length: 816\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_dir = '/home/chris/projects/quest-new/learning/data/features/ems_output/'\n",
      "\n",
      "# use pandas to enable super powers\n",
      "import math as math\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# TODO: use nose for testing\n",
      "\n",
      "data_instances = pd.io.parsers.read_table('/home/chris/projects/quest-new/output/wmt2014/source.en.tok_to_target.es.tok.out')\n",
      "# Working - add the y values (training scores) as the final column\n",
      "\n",
      "# SCORES\n",
      "# WMT 2012 data\n",
      "train_scores = pd.io.parsers.read_table('/home/chris/projects/random_indexing/python/wmt2014/quality_estimation/data/wmt_2012_data/QualityEstimation/training_set_annotations/target_system.effort')\n",
      "\n",
      "# WMT 2014 data \n",
      "#train_scores = pd.io.parsers.read_table('/home/chris/projects/random_indexing/python/wmt2014/quality_estimation/data/perceived_PE_effort/task1-1_en-es_training/en-es_score.train')\n",
      "\n",
      "\n",
      "data_instances['train_scores'] = train_scores\n",
      "\n",
      "#print (data_instances.shape)\n",
      "\n",
      "# shuffle data randomly \n",
      "# 10% test data \n",
      "def split_data(complete_frame, test_percent_split):\n",
      "    x_size = complete_frame.shape[0]\n",
      "    test_data_size = math.floor(x_size / test_percent_split)\n",
      "\n",
      "    test_rows = np.random.choice(complete_frame.index.values, test_data_size)\n",
      "\n",
      "    test_data = complete_frame.ix[test_rows]  \n",
      "    train_data = complete_frame.drop(test_rows)\n",
      "\n",
      "    return { 'train': train_data, 'test': test_data }\n",
      "\n",
      "train_test = split_data(data_instances, 10)\n",
      "#print ('train shape: %s' % str(train_test['train'].shape))\n",
      "#print ('test shape: %s' % str(train_test['test'].shape))\n",
      "# train_test\n",
      "\n",
      "# call the scikit-learn code here\n",
      "\n",
      "\n",
      "print(train_test['train']['train_scores'])\n",
      "\n",
      "#data_instances\n",
      "# Working - get quest to output the feature index descriptions \n",
      "\n",
      "\n",
      "# Update -- feature indexing works, but quest prints with a trailing whitespace, so the index loading is still messed up"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0     4.3\n",
        "1     4.0\n",
        "2     4.0\n",
        "3     3.8\n",
        "4     4.0\n",
        "5     4.3\n",
        "6     3.0\n",
        "7     5.0\n",
        "8     4.3\n",
        "9     3.0\n",
        "10    3.8\n",
        "12    3.8\n",
        "13    4.0\n",
        "14    4.3\n",
        "15    3.7\n",
        "...\n",
        "1816    3.7\n",
        "1817    2.0\n",
        "1818    2.5\n",
        "1820    3.0\n",
        "1821    3.5\n",
        "1822    4.5\n",
        "1823    4.3\n",
        "1824    5.0\n",
        "1825    4.0\n",
        "1826    5.0\n",
        "1827    3.2\n",
        "1828    5.0\n",
        "1829    3.2\n",
        "1830    5.0\n",
        "1831    NaN\n",
        "Name: train_scores, Length: 1655, dtype: float64\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:31: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def set_learning_method(config, X_train, y_train):\n",
      "    \"\"\"\n",
      "    Instantiates the sklearn's class corresponding to the value set in the \n",
      "    configuration file for running the learning method.\n",
      "    \n",
      "    @param config: configuration object\n",
      "    @return: an estimator with fit() and predict() methods\n",
      "    \"\"\"\n",
      "    estimator = None\n",
      "    \n",
      "    learning_cfg = config.get(\"learning\", None)\n",
      "    if learning_cfg:\n",
      "        p = learning_cfg.get(\"parameters\", None)\n",
      "        o = learning_cfg.get(\"optimize\", None)\n",
      "        scorers = \\\n",
      "        set_scorer_functions(learning_cfg.get(\"scorer\", ['mae', 'rmse']))\n",
      "        \n",
      "        method_name = learning_cfg.get(\"method\", None)\n",
      "        if method_name == \"SVR\":\n",
      "            if o:\n",
      "                tune_params = set_optimization_params(o)\n",
      "                estimator = optimize_model(SVR(), X_train, y_train, \n",
      "                                          tune_params, \n",
      "                                          scorers, \n",
      "                                          o.get(\"cv\", 5),\n",
      "                                          o.get(\"verbose\", True),\n",
      "                                          o.get(\"n_jobs\", 1))\n",
      "                \n",
      "            elif p:\n",
      "                estimator = SVR(C=p.get(\"C\", 10),\n",
      "                                epsilon=p.get('epsilon', 0.01),\n",
      "                                kernel=p.get('kernel', 'rbf'),\n",
      "                                degree=p.get('degree', 3),\n",
      "                                gamma=p.get('gamma', 0.0034),\n",
      "                                tol=p.get('tol', 1e-3),\n",
      "                                verbose=False)\n",
      "            else:\n",
      "                estimator = SVR()\n",
      "        \n",
      "        elif method_name == \"SVC\":\n",
      "            if o:\n",
      "                tune_params = set_optimization_params(o)\n",
      "                estimator = optimize_model(SVC(), X_train, y_train,\n",
      "                                           tune_params,\n",
      "                                           scorers,\n",
      "                                           o.get('cv', 5),\n",
      "                                           o.get('verbose', True),\n",
      "                                           o.get('n_jobs', 1))\n",
      "                \n",
      "            elif p:\n",
      "                estimator = SVC(C=p.get('C', 1.0),\n",
      "                                kernel=p.get('kernel', 'rbf'), \n",
      "                                degree=p.get('degree', 3),\n",
      "                                gamma=p.get('gamma', 0.0),\n",
      "                                coef0=p.get('coef0', 0.0),\n",
      "                                tol=p.get('tol', 1e-3),\n",
      "                                verbose=p.get('verbose', False))\n",
      "            else:\n",
      "                estimator = SVC()\n",
      "                    \n",
      "        elif method_name == \"LassoCV\":\n",
      "            if p:\n",
      "                estimator = LassoCV(eps=p.get('eps', 1e-3),\n",
      "                                    n_alphas=p.get('n_alphas', 100),\n",
      "                                    normalize=p.get('normalize', False),\n",
      "                                    precompute=p.get('precompute', 'auto'),\n",
      "                                    max_iter=p.get('max_iter', 1000),\n",
      "                                    tol=p.get('tol', 1e-4),\n",
      "                                    cv=p.get('cv', 10),\n",
      "                                    verbose=False)\n",
      "            else:\n",
      "                estimator = LassoCV()\n",
      "        \n",
      "        elif method_name == \"LassoLars\":\n",
      "            if o:\n",
      "                tune_params = set_optimization_params(o)\n",
      "                estimator = optimize_model(LassoLars(), X_train, y_train, \n",
      "                                          tune_params,\n",
      "                                          scorers,\n",
      "                                          o.get(\"cv\", 5),\n",
      "                                          o.get(\"verbose\", True),\n",
      "                                          o.get(\"n_jobs\", 1))\n",
      "                \n",
      "            if p:\n",
      "                estimator = LassoLars(alpha=p.get('alpha', 1.0),\n",
      "                                      fit_intercept=p.get('fit_intercept', True),\n",
      "                                      verbose=p.get('verbose', False),\n",
      "                                      normalize=p.get('normalize', True),\n",
      "                                      max_iter=p.get('max_iter', 500),\n",
      "                                      fit_path=p.get('fit_path', True))\n",
      "            else:\n",
      "                estimator = LassoLars()\n",
      "        \n",
      "        elif method_name == \"LassoLarsCV\":\n",
      "            if p:\n",
      "                estimator = LassoLarsCV(max_iter=p.get('max_iter', 500),\n",
      "                                        normalize=p.get('normalize', True),\n",
      "                                        max_n_alphas=p.get('max_n_alphas', 1000),\n",
      "                                        n_jobs=p.get('n_jobs', 1),\n",
      "                                        cv=p.get('cv', 10),\n",
      "                                        verbose=False)\n",
      "            else:\n",
      "                estimator = LassoLarsCV()\n",
      "                \n",
      "    return estimator, scorers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the quest ML dependencies\n",
      "\n",
      "from evaluation_measures import root_mean_squared_error, mean_absolute_error\n",
      "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.linear_model.coordinate_descent import LassoCV\n",
      "from sklearn.linear_model.least_angle import LassoLarsCV, LassoLars\n",
      "from sklearn.linear_model.randomized_l1 import RandomizedLasso\n",
      "from sklearn.linear_model.randomized_l1 import RandomizedLasso\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "from sklearn.metrics.metrics import mean_squared_error, f1_score, \\\n",
      "    precision_score, recall_score\n",
      "from sklearn.svm.classes import SVR, SVC\n",
      "from sklearn_utils import scale_datasets, open_datasets, assert_number, \\\n",
      "    assert_string\n",
      "    \n",
      "import logging as log\n",
      "    \n",
      "# util function for scoring\n",
      "def set_scorer_functions(scorers):\n",
      "    scores = []\n",
      "    for score in scorers:\n",
      "        if score == 'mae':\n",
      "            scores.append((score, mean_absolute_error))\n",
      "        elif score == 'rmse':\n",
      "            scores.append((score, root_mean_squared_error))\n",
      "        elif score == 'mse':\n",
      "            scores.append((score, mean_squared_error))\n",
      "        elif score == 'f1_score':\n",
      "            scores.append((score, f1_score))\n",
      "        elif score == 'precision_score':\n",
      "            scores.append((score, precision_score))\n",
      "        elif score == 'recall_score':\n",
      "            scores.append((score, recall_score))\n",
      "        elif score == 'pearson_corrcoef':\n",
      "            scores.append((score, pearson_corrcoef))\n",
      "        elif score == 'binary_precision':\n",
      "            scores.append((score, binary_precision))\n",
      "            \n",
      "    return scores\n",
      "\n",
      "# sets the selection method\n",
      "#transformer = set_selection_method(config)\n",
      "\n",
      "# if the system is configured to run feature selection\n",
      "# runs it and modifies the datasets to the new dimensions\n",
      "#     if transformer is not None:\n",
      "#         log.info(\"Running feature selection %s\" % str(transformer))\n",
      "        \n",
      "#         log.debug(\"X_train dimensions before fit_transform(): %s,%s\" % X_train.shape)\n",
      "#         log.debug(\"y_train dimensions before fit_transform(): %s\" % y_train.shape)\n",
      "        \n",
      "#         X_train = transformer.fit_transform(X_train, y_train)\n",
      "        \n",
      "#         log.debug(\"Dimensions after fit_transform(): %s,%s\" % X_train.shape)\n",
      "        \n",
      "#         if X_test is not None:\n",
      "#             X_test = transformer.transform(X_test)\n",
      "\n",
      "\n",
      "# sets learning algorithm and runs it over the training data\n",
      "#estimator, scorers = set_learning_method(config, X_train, y_train)\n",
      "\n",
      "#     scores = []\n",
      "#     for score in scorers:\n",
      "#         if score == 'mae':\n",
      "#             scores.append((score, mean_absolute_error))\n",
      "#         elif score == 'rmse':\n",
      "#             scores.append((score, root_mean_squared_error))\n",
      "#         elif score == 'mse':\n",
      "#             scores.append((score, mean_squared_error))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_test = split_data(data_instances, 10)\n",
      "    \n",
      "# Chris - some default params\n",
      "\n",
      "# TODO - try different estimators\n",
      "estimator = SVR()\n",
      "#estimator = SVC()\n",
      "#estimator = LinearRegression()\n",
      "\n",
      "#estimator = LassoLars()\n",
      "scorers = [('mae', mean_absolute_error), ('rmse', root_mean_squared_error)]\n",
      "\n",
      "X_train = train_test['train'][[col for col in train_test['train'].columns if col not in ['train_scores']]]\n",
      "X_test = train_test['test'][[col for col in train_test['test'].columns if col not in ['train_scores']]]\n",
      "\n",
      "y_train = train_test['train']['train_scores']\n",
      "y_test = train_test['test']['train_scores']\n",
      "\n",
      "log.info(\"Running learning algorithm %s\" % str(estimator))\n",
      "estimator.fit(X_train, y_train)\n",
      "\n",
      "if (X_test is not None) and (y_test is not None):\n",
      "    log.info(\"Predicting unseen data using the trained model...\")\n",
      "    print(\"Predicting unseen data using the trained model...\")\n",
      "    y_hat = estimator.predict(X_test)\n",
      "    log.info(\"Evaluating prediction on the test set...\")\n",
      "    for scorer_name, scorer_func in scorers:\n",
      "        v = scorer_func(y_test, y_hat)\n",
      "        log.info(\"%s = %s\" % (scorer_name, v))\n",
      "    log.info(\"Customized scores: \")\n",
      "    try:\n",
      "        log.info(\"pearson_corrcoef = %s\" % pearson_corrcoef(y_test,  y_hat))\n",
      "    except:\n",
      "        pass\n",
      "    try:\n",
      "        log.info(\"Precision score: = %s\" % precision_score(y_test, y_hat))\n",
      "    except:\n",
      "        pass\n",
      "    try:\n",
      "        log.info(\"Recall score: = %s\" % recall_score(y_test, y_hat))\n",
      "    except:\n",
      "        pass\n",
      "    try:\n",
      "        log.info(\"F1 score: = %s\" % f1_score(y_test, y_hat))\n",
      "    except:\n",
      "        pass\n",
      "    try:\n",
      "        log.info(\"MAE: = %s\" % mean_absolute_error(y_test, y_hat))\n",
      "        print(\"MAE: = %s\" % mean_absolute_error(y_test, y_hat))\n",
      "    except:\n",
      "        pass\n",
      "    try:\n",
      "        log.info(\"RMSE: = %s\" % root_mean_squared_error(y_test, y_hat))\n",
      "        print(\"RMSE: = %s\" % root_mean_squared_error(y_test, y_hat))\n",
      "    except:\n",
      "        pass\n",
      "#     try:\n",
      "#         res = classify_report_bin(y_test, y_hat)\n",
      "#         if \"N/A\" <> res:\n",
      "#             log.info(\"Classify report bin: = %s\" % res)\n",
      "#         else:\n",
      "#             res = classify_report_bin_regression(y_test, y_hat)\n",
      "#             if \"N/A\" <> res:\n",
      "#                 log.info(\"Classify report bin regression: = %s\" % res)\n",
      "#             else:\n",
      "#                 if ref_thd is None:\n",
      "#                     log.error(\"No ref thd defined\")\n",
      "#                 else:\n",
      "#                     refthd = float(ref_thd)\n",
      "#                     res = classify_report_regression(y_test, y_hat, refthd)\n",
      "#                     log.info(\"Classify report regression: = %s\" % res)\n",
      "#     except Exception, e:\n",
      "#         print e\n",
      "#     with open(\"predicted.csv\", 'w') as _fout:\n",
      "#         for _x,  _y in zip(y_test, y_hat):\n",
      "#             print >> _fout,  \"%f\\t%f\" % (_x,  _y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predicting unseen data using the trained model...\n",
        "MAE: = 0.688560111723"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "RMSE: = 0.846175459716\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:31: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO - error analysis - which segs are wrong?\n",
      "# Data selection based on random indexing - this is another good paper idea\n",
      "# Domain adapation with Random indexing?\n",
      "\n",
      "WMT12 baseline system - CMU paper\n",
      "0.69 - 0.82 \n",
      "\n",
      "# WMT 2014 experiments \n",
      "Predicting unseen data using the trained model...\n",
      "MAE: = 0.241694973346\n",
      "RMSE: = 0.317832032721\n",
      "\n",
      "# with all 2014 en-es data - we expect scores to change because of the cross-validation    \n",
      "Predicting unseen data using the trained model...\n",
      "MAE: = 0.490180458094\n",
      "RMSE: = 0.677979892434    \n",
      "    \n",
      "Predicting unseen data using the trained model...\n",
      "MAE: = 0.501959425242\n",
      "RMSE: = 0.694692142869\n",
      "    \n",
      "    \n",
      "# WMT 2012 -- Baseline stuff\n",
      "Predicting unseen data using the trained model...\n",
      "MAE: = 0.828479123721\n",
      "RMSE: = 1.00643779137\n",
      "    \n",
      "Predicting unseen data using the trained model...\n",
      "MAE: = 0.776307444845\n",
      "RMSE: = 0.9318582504\n",
      "\n",
      "# GOT IT - with baseline 79 features file (one is commented)\n",
      "Predicting unseen data using the trained model...\n",
      "MAE: = 0.695741608119\n",
      "RMSE: = 0.838263363242\n",
      "# above was with SVR\n",
      "\n",
      "# now with LassoLars - TODO: LassoLars throws NaN error\n",
      "    \n",
      "    \n",
      "# TODO - how to check if my results are better than random?\n",
      "    \n",
      "# from  quest\n",
      "def run(config):\n",
      "    '''\n",
      "    Runs the main code of the program. Checks for mandatory parameters, opens\n",
      "    input files and performs the learning steps.\n",
      "    '''\n",
      "    # check if the mandatory parameters are set in the config file\n",
      "    x_train_path = config.get(\"x_train\", None)\n",
      "    if not x_train_path:\n",
      "        msg = \"'x_train' option not found in the configuration file. \\\n",
      "        The training dataset is mandatory.\"\n",
      "        raise Exception(msg)\n",
      "\n",
      "    y_train_path = config.get(\"y_train\", None)\n",
      "    if not y_train_path:\n",
      "        msg = \"'y_train' option not found in the configuration file. \\\n",
      "        The training dataset is mandatory.\"\n",
      "        raise Exception(msg)\n",
      "        \n",
      "    learning = config.get(\"learning\", None)\n",
      "    if not learning:\n",
      "        msg = \"'learning' option not found. At least one \\\n",
      "        learning method must be set.\"\n",
      "        raise Exception(msg)\n",
      "    \n",
      "    # checks for the optional parameters\n",
      "    x_test_path = config.get(\"x_test\", None)\n",
      "    y_test_path = config.get(\"y_test\", None)\n",
      "\n",
      "    separator = config.get(\"separator\", DEFAULT_SEP)\n",
      "    \n",
      "    labels_path = config.get(\"labels\", None)\n",
      "        \n",
      "    scale = True\n",
      "    if scale:\n",
      "        # preprocess and execute mean removal\n",
      "        X_train, X_test = scale_datasets(X_train, X_test)\n",
      "    \n",
      "    log.info(\"Opening input files ...\")\n",
      "    log.debug(\"X_train: %s\" % x_train_path)\n",
      "    log.debug(\"y_train: %s\" % y_train_path)\n",
      "    log.debug(\"X_test: %s\" % x_test_path)\n",
      "    log.debug(\"y_test_path: %s\" % y_test_path)\n",
      "\n",
      "    # open feature and response files    \n",
      "    X_train, y_train, X_test, y_test, labels = \\\n",
      "    open_datasets(x_train_path, y_train_path, x_test_path,\n",
      "                  y_test_path, separator, labels_path)\n",
      "\n",
      "   \n",
      "\n",
      "    # fits training data and predicts the test set using the trained model\n",
      "    y_hat = fit_predict(X_train, y_train, X_test, y_test, config.get(\"ref_thd\", None))\n",
      "    \n",
      "# working - modify fit predict\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# TODO: store the results in csv with some metadata about the experimental params\n",
      "# ml_directory = '/home/chris/programs/quest/learning/\n",
      "# python src/learn_model.py config/svr.yaml\n",
      "%run src/learn_model.py config/svr.cfg\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:Opening input files ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:Scaling datasets...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:Tuning hyper-parameters for mae\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/chris/programs/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:466: DeprecationWarning: Passing a loss function is deprecated and will be removed in 0.15. Either use strings or score objects.The relevant new parameter is called ''scoring''. \n",
        "  self.loss_func, self.score_func, self.scoring)\n",
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.6s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   18.0s finished\n",
        "INFO:root:Best parameters set found on development set:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:{'epsilon': 0.10000000000000001, 'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:Tuning hyper-parameters for rmse\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kernel ['rbf']\n",
        "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
        "Fitting 3 folds for each of 8 candidates, totalling 24 fits"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/chris/programs/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:466: DeprecationWarning: Passing a loss function is deprecated and will be removed in 0.15. Either use strings or score objects.The relevant new parameter is called ''scoring''. \n",
        "  self.loss_func, self.score_func, self.scoring)\n",
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.6s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   18.0s finished\n",
        "INFO:root:Best parameters set found on development set:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:{'epsilon': 0.10000000000000001, 'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:Running learning algorithm SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.01,\n",
        "  kernel=rbf, max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:Predicting unseen data using the trained model...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:Evaluating prediction on the test set...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:mae = 0.46500703813\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:rmse = 0.659793355871\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output files\n",
      "training = output_dir + 'training-features.out'\n",
      "test = output_dir + 'test-features.out'\n",
      "# this cell formats and prints the training features\n",
      "with open(training,'w') as f:\n",
      "    for row in new_train:\n",
      "        print (row, file=f)\n",
      "        \n",
      "with open(test,'w') as f:\n",
      "    for row in new_test:\n",
      "        print (row, file=f)\n",
      "\n",
      "\n",
      "# TODO: try other classifiers -- get some better intuition about the performance\n",
      "# LR, NB to start\n",
      "# Neural network, etc..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "u'/home/chris/programs/quest/learning'"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run 1 with extra feature:\n",
      "# INFO:root:Predicting unseen data using the trained model...\n",
      "# INFO:root:Evaluating prediction on the test set...\n",
      "# INFO:root:mae = 0.412518890672\n",
      "# INFO:root:rmse = 0.602574628609\n",
      "\n",
      "# run 2 WITHOUT extra feature:\n",
      "# INFO:root:Predicting unseen data using the trained model...\n",
      "# INFO:root:Evaluating prediction on the test set...\n",
      "# INFO:root:mae = 0.412413880436\n",
      "# INFO:root:rmse = 0.602584789385\n",
      "\n",
      "# run 3 JUST extra feature:\n",
      "# INFO:root:Predicting unseen data using the trained model...\n",
      "# INFO:root:Evaluating prediction on the test set...\n",
      "# INFO:root:mae = 0.465217126314\n",
      "# INFO:root:rmse = 0.659930041776\n",
      "\n",
      "# run 4 sanity check - random features:\n",
      "# INFO:root:Predicting unseen data using the trained model...\n",
      "# INFO:root:Evaluating prediction on the test set...\n",
      "# INFO:root:mae = 0.465236937727\n",
      "# INFO:root:rmse = 0.659939032088\n",
      "\n",
      "# random results confirmed:\n",
      "# INFO:root:Predicting unseen data using the trained model...\n",
      "# INFO:root:Evaluating prediction on the test set...\n",
      "# INFO:root:mae = 0.465119763497\n",
      "# INFO:root:rmse = 0.659865420079\n",
      "\n",
      "# COMING BACK - after zeroing the corpus indicies for cells which are only 1 or -1\n",
      "# finished indexing tokens...\n",
      "# similarity between vecs for: oro and gold is: 0.504181\n",
      "# similarity between vecs for: oro and covet is: 0.965443\n",
      "# before zeroing: \n",
      "# INDEX: 4798 - Source and target sentence similarity after random indexing: 0.674727\n",
      "# INDEX: 4798 - Source similarity with a random target (index 4155): 0.699771\n",
      "# after zeroing: \n",
      "# INDEX: 4798 - Source and target sentence similarity after random indexing: 0.674727\n",
      "# INDEX: 4798 - Source similarity with a random target (index 14408): 0.883923\n",
      "\n",
      "# with the new (index-zeroed) feature\n",
      "# INFO:root:Predicting unseen data using the trained model...\n",
      "# INFO:root:Evaluating prediction on the test set...\n",
      "# INFO:root:mae = 0.412617446921\n",
      "# INFO:root:rmse = 0.602652127017\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}